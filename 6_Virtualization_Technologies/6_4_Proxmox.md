| **Inicio**         | **atr√°s 3**         | **Siguiente 5**           |
| ------------------ | ------------------- | ------------------------- |
| [üè†](../README.md) | [‚è™](./6_3_ESXi.md) | [‚è©](./6_5_Hypervisor.md) |

---

## **√çndice**

| Temario                                                                          |
| -------------------------------------------------------------------------------- |
| [152. Proxmox](#152-proxmox)                                                     |
| [153. Entorno virtual Proxmox](#153-entorno-virtual-proxmox)                     |
| [154. ¬øQu√© es la virtualizaci√≥n Proxmox?](#154-qu√©-es-la-virtualizaci√≥n-proxmox) |

# **Proxmox**

## **152. Proxmox**

### ¬øQu√© es **Proxmox VE**?

**Proxmox VE (Virtual Environment)** es una plataforma de virtualizaci√≥n de c√≥digo abierto basada en Debian que integra dos tecnolog√≠as principales:

- **KVM/QEMU** para m√°quinas virtuales (VMs) ‚Äî virtualizaci√≥n completa (como VMware ESXi).
- **LXC** para contenedores ligeros (CTs) ‚Äî aislamiento a nivel de sistema operativo.

Proxmox agrega una **interfaz web**, **API/CLI**, clustering nativo, gesti√≥n de almacenamiento (ZFS, Ceph, LVM, NFS, etc.), backups, HA y herramientas para orquestar todo en un datacenter peque√±o/mediano.

### Componentes clave (resumen)

- **pve-manager**: el servicio que corre la interfaz web y la gesti√≥n central.
- **QEMU/KVM**: motor de VMs.
- **LXC**: motor de contenedores.
- **pve-cluster / corosync**: clustering y sincronizaci√≥n entre nodos.
- **pve-storage**: integraci√≥n de m√∫ltiples tipos de almacenamiento (local, LVM, ZFS, NFS, Ceph).
- **vzdump / Proxmox Backup Server (PBS)**: backups de VMs/CTs; PBS ofrece deduplicaci√≥n y backups incrementales.
- **GUI web**: consola gr√°fica (https\://IP_del_nodo:8006) para administrar todo.
- **CLI / API**: `qm`, `pct`, `pvecm`, `pvesh` para automatizaci√≥n.

### ¬øC√≥mo funciona (arquitectura b√°sica)?

1. **Instalas Proxmox VE** en un servidor (recomendado con la ISO oficial).
2. Creas VMs (KVM) o CTs (LXC).
3. El host gestiona recursos (CPU, memoria, I/O) y presenta redes virtuales (bridges) y datastores.
4. Al agregar m√°s nodos, los unes en un **cluster** (pvecm), que permite migrar VMs entre nodos, HA y gesti√≥n centralizada.
5. Para almacenamiento distribuido a escala, integras **Ceph** o **vSAN-like** (ZFS en cada nodo + sincronizaci√≥n).

### Ejemplos pr√°cticos ‚Äî pasos y comandos √∫tiles

> **Nota**: muchos usuarios administran Proxmox desde la **Web GUI** (m√°s sencillo). Abajo tienes flujos equivalentes por **CLI**.

#### 1) Instalar Proxmox

- M√©todo recomendado: bajar la **ISO oficial de Proxmox VE**, crear USB booteable e instalar.
- Alternativa avanzada: instalar sobre Debian y a√±adir repositorios Proxmox (para entornos controlados).

#### 2) Crear una VM desde la Web UI

1. Subir ISO: _Datacenter ‚Üí Storage ‚Üí ISO Images ‚Üí Upload_.
2. _Create VM_ ‚Üí elegir ID/nombre ‚Üí resources (CPU/RAM) ‚Üí disco (local-lvm / ZFS) ‚Üí network (bridge vmbr0) ‚Üí finish ‚Üí _Start_ ‚Üí abrir consola y proceder a la instalaci√≥n del OS.

#### 3) Crear una VM desde CLI (`qm`)

Ejemplo (crea VM id 100, 4 GB RAM, 2 cores, disco 32 GB, ISO como CD):

```bash
# Crear VM y asignar recursos
qm create 100 --name ubuntu22 --memory 4096 --cores 2 --net0 virtio,bridge=vmbr0

# Crear disco y asignarlo como scsi0 (32G thin)
qm set 100 --scsihw virtio-scsi-pci --scsi0 local-lvm:32

# Adjuntar ISO como CD (suponiendo que subiste iso a storage 'local')
qm set 100 --ide2 local:iso/ubuntu-22.04.iso,media=cdrom

# Configurar arranque
qm set 100 --boot c --bootdisk scsi0

# Iniciar VM
qm start 100
```

#### 4) Crear un contenedor LXC con `pct`

Ejemplo (ID 101, plantilla de Ubuntu, 2 GB RAM, rootfs 8GB, net dhcp):

```bash
pct create 101 local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.gz \
  --hostname ct01 --cores 2 --memory 2048 --rootfs local-lvm:8 \
  --net0 name=eth0,bridge=vmbr0,ip=dhcp --start 1
```

`pct start 101`, `pct enter 101` para entrar dentro del contenedor.

#### 5) Networking: configuraci√≥n de puente (bridge)

Ejemplo `/etc/network/interfaces` (proxmox/debian style):

```text
auto vmbr0
iface vmbr0 inet static
    address 192.168.1.10/24
    gateway 192.168.1.1
    bridge-ports eno1
    bridge-stp off
    bridge-fd 0
```

VLAN en una VM: en net0 agregar `tag=100`:

```
net0: virtio=DE:AD:BE:EF:00:01,bridge=vmbr0,tag=100
```

#### 6) ZFS como almacenamiento (ejemplo r√°pido)

En la instalaci√≥n puedes elegir ZFS. Para crear un pool manualmente:

```bash
zpool create -f rpool mirror /dev/sda /dev/sdb
```

Proxmox detectar√° `rpool` y podr√°s agregarlo como storage para VMs/CTs.

#### 7) Backups con `vzdump`

Backup de VM 100 en modo snapshot al storage `backup` con compresi√≥n zstd:

```bash
vzdump 100 --mode snapshot --compress zstd --storage backup
```

Restore (ej. QEMU backup) con `qmrestore`:

```bash
qmrestore /var/lib/vz/dump/vzdump-qemu-100-2025_09_02-00-00.vma.zst 200 --storage local-lvm
```

#### 8) Migraci√≥n en vivo (live migration)

Desde CLI (requiere shared storage o use online migration with storage migration):

```bash
qm migrate 100 nodo-destino --online
```

#### 9) Crear un cluster b√°sico

En el **primer** nodo:

```bash
pvecm create mi-cluster
```

En nodos secundarios:

```bash
pvecm add IP_DEL_PRIMER_NODO
```

Despu√©s de esto los nodos comparten configuraci√≥n; puedes administrar VMs desde cualquier nodo.

### Alta disponibilidad (HA) y Ceph

- **HA**: Proxmox utiliza `pvecm` + `corosync` para coordinar estado; al habilitar HA para una VM, si el host falla la VM se reiniciar√° en otro nodo disponible (requiere almacenamiento compartido o Ceph).
- **Ceph**: integraci√≥n nativa (CephFS/RBD) para almacenamiento distribuido altamente disponible. Proxmox facilita la creaci√≥n de cl√∫steres Ceph.

### Proxmox Backup Server (PBS)

- Soluci√≥n complementaria para backups con deduplicaci√≥n, cifrado y backups incrementales.
- Flujo t√≠pico: Proxmox VE ‚Üí storage backup via PBS ‚Üí restores r√°pidas y deduplicadas.

### Seguridad y gesti√≥n

- **Autenticaci√≥n**: usuarios locales, LDAP/AD, 2FA (TOTP).
- **Firewall**: PVE firewall por host y por VM/CT.
- **Actualizaciones**: usa repositorio enterprise si tienes suscripci√≥n; para community ed., usar repositorio no-subscription (o habilitar repositorios p√∫blicos).
- **SSH**: usar llaves, deshabilitar root-password sin llave, limitar acceso.

### Diferencias frente a otras soluciones (breve)

- A diferencia de VMware ESXi, Proxmox es **open source** y combina **VMs + contenedores** nativamente; ZFS y Ceph est√°n integrados; tiene una **web UI** potente y API REST complet√≠sima. Es ideal para POCs, labs, SMB y producci√≥n con menor coste de licenciamiento.

### Buenas pr√°cticas / recomendaciones

- Usa **ZFS** o almacenamiento redundante para producci√≥n (RAID/ZFS/CEPH).
- Separa tr√°fico: management, storage, vMotion en NICs/VLANs separadas.
- Haz **backups regulares** y testea restores.
- No mantener snapshots muy largos.
- Monitoriza con Prometheus/Grafana o las m√©tricas integradas.
- Eval√∫a Proxmox Backup Server para deduplicaci√≥n y eficiencia en backups.

---

[üîº](#√≠ndice)

---

## **153. Entorno virtual Proxmox**

### 1) ¬øQu√© es el _entorno virtual Proxmox_?

Proxmox VE (Virtual Environment) es una plataforma de virtualizaci√≥n **open-source** basada en Debian que integra:

- **KVM/QEMU** para m√°quinas virtuales completas (VMs).
- **LXC** para contenedores ligeros (CTs).
- Una **interfaz web** (GUI), API REST y herramientas CLI para gestionar todo (VMs, contenedores, almacenamiento, red, cluster, backups, HA, etc.).

Proxmox permite administrar nodos individuales o cl√∫sters completos, soporta m√∫ltiples backends de almacenamiento (ZFS, LVM, NFS, Ceph, iSCSI, etc.) y se integra con **Proxmox Backup Server (PBS)** para backups eficientes.

### 2) Componentes y arquitectura (resumen)

- **Nodo:** servidor f√≠sico con Proxmox VE instalado.
- **Cluster:** conjunto de nodos coordinados (pvecm / corosync) que comparten configuraci√≥n y permiten migraciones/HA.
- **pve-manager:** servicio que corre la GUI/API.
- **QEMU/KVM & LXC:** motores para VMs y containers.
- **Storage:** `local`, `local-lvm`, `ZFS`, `Ceph/RBD`, `NFS`, `iSCSI`, etc.
- **Networking:** bridges (`vmbr0`), bonding, VLANs (tag), SDN opcional.
- **Backup:** `vzdump` (builtin) y PBS (deduplicaci√≥n/incrementales).

### 3) Instalaci√≥n r√°pida (pasos principales)

1. Descarga la ISO oficial de Proxmox VE.
2. Crea USB booteable (Rufus, dd).
3. Bootea el servidor e instala (opci√≥n de ZFS es t√≠pica para root).
4. Configura IP de gesti√≥n, hostname, DNS y NTP en el instalador.
5. Accede a la GUI: `https://IP_DEL_NODO:8006` y entra con `root` y la contrase√±a creada.

**Tip:** si vas a cl√∫ster, aseg√∫rate de que todos los nodos tengan nombres DNS resolubles y hora sincronizada (NTP).

### 4) Crear VMs y CTs ‚Äî ejemplos pr√°cticos

#### 4.1 Crear VM (CLI con `qm`)

```bash
# Crear VM id=100, 2 cores, 4GB RAM, interfaz virtio
qm create 100 --name ubuntu22 --memory 4096 --cores 2 \
  --net0 virtio,bridge=vmbr0

# Crear disco 32GB en storage local-lvm y asignarlo (scsi)
qm set 100 --scsihw virtio-scsi-pci --scsi0 local-lvm:32

# Adjuntar ISO (subida a storage local:iso/)
qm set 100 --ide2 local:iso/ubuntu-22.04.iso,media=cdrom

# Arrancar VM
qm start 100
```

Luego abre la consola desde la GUI (`https://IP:8006`) y procede con la instalaci√≥n del SO.

#### 4.2 Crear LXC container (CLI con `pct`)

```bash
# Asume que subiste plantilla a local:vztmpl/
pct create 101 local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.gz \
  --hostname ct01 --cores 2 --memory 2048 --rootfs local-lvm:8 \
  --net0 name=eth0,bridge=vmbr0,ip=dhcp --start 1
```

`pct enter 101` para acceder al contenedor desde el host.

### 5) Storage: opciones y ejemplos

- **local-lvm**: r√°pido para VMs en un √∫nico host (thin provisioning).
- **ZFS**: snapshot, compresi√≥n, env√≠o/replicaci√≥n (muy com√∫n en Proxmox).

  ```bash
  # ejemplo: crear zpool (simple)
  zpool create -f rpool mirror /dev/sda /dev/sdb
  ```

- **Ceph**: almacenamiento distribuido, recomendado para cl√∫sters con alta disponibilidad.
- **NFS / iSCSI**: √∫til para storage compartido sin Ceph.
- Comprueba los storages con:

  ```bash
  pvesm status
  ```

### 6) Networking: bridges, VLANs y bonding (ejemplos)

#### /etc/network/interfaces (ejemplo b√°sico con bridge + bond)

```text
auto lo
iface lo inet loopback

auto eno1
iface eno1 inet manual

auto eno2
iface eno2 inet manual

auto bond0
iface bond0 inet manual
    bond-slaves eno1 eno2
    bond-miimon 100
    bond-mode 802.3ad

auto vmbr0
iface vmbr0 inet static
    address 192.168.10.10/24
    gateway 192.168.10.1
    bridge-ports bond0
    bridge-stp off
    bridge-fd 0
```

#### VLAN en una VM (net0)

En la GUI o con `qm set`:

```
net0: virtio=DE:AD:BE:EF:00:01,bridge=vmbr0,tag=100
```

Eso coloca la interfaz en VLAN 100.

### 7) Cl√∫ster y migraci√≥n en vivo

#### Crear cl√∫ster (nodo maestro)

```bash
pvecm create mi-cluster
```

#### A√±adir nodo secundario (desde nodo secundario)

```bash
pvecm add IP_NODO_PRIMARIO
```

**Requisitos:** hostnames resolubles, NTP sincronizado, puertos corosync abiertos (por defecto 5404/5405) y buena conectividad.

#### Migrar VM en caliente

```bash
qm migrate 100 nodo-destino --online
```

(Para migraci√≥n online normalmente necesitas storage compartido o usar almacenamiento migrado online).

### 8) Alta disponibilidad (HA)

- Una vez en cl√∫ster, habilitas HA (desde GUI ‚Üí Datacenter ‚Üí HA) para VMs que requieres que se reinicien en otros nodos si el host falla.
- **Requisito**: almacenamiento compartido (Ceph, NFS, iSCSI) o replicaci√≥n/sincronizaci√≥n adecuada; de lo contrario la VM no ser√° reiniciable en otro host con su disco.

### 9) Backups y restores

#### Hacer backup (vzdump)

```bash
# Backup de VM 100 con snapshot, compresi√≥n zstd al storage 'backup'
vzdump 100 --mode snapshot --compress zstd --storage backup
```

#### Restaurar VM

```bash
qmrestore /var/lib/vz/dump/vzdump-qemu-100-2025_09_02-00-00.vma.zst 200 --storage local-lvm
```

#### Proxmox Backup Server (PBS)

- Recomendado para backups incrementales y deduplicados; se integra nativamente como storage en la GUI.

### 10) Monitorizaci√≥n y automatizaci√≥n

- GUI muestra m√©tricas, logs y tasks.
- CLI/API: `pvesh` (API shell) y `pvesm`, `qm`, `pct` para scripting.
  Ejemplo: listar nodos v√≠a API:

  ```bash
  pvesh get /nodes
  ```

- Integra con **Prometheus + Grafana** (exporter disponibles) para monitorizaci√≥n avanzada.

### 11) Seguridad & buenas pr√°cticas

- Usa **SSH keys**, deshabilita root password SSH cuando sea posible.
- Habilita **2FA** en la GUI (TOTP).
- Separa redes: management, storage, VM traffic, backup.
- Usa ZFS/RAID/Ceph para redundancia.
- Testea restores de backup peri√≥dicamente.
- Aplica actualizaciones desde repositorios oficiales (si es producci√≥n, usa repos. enterprise o ventana de mantenimiento).

### 12) Diferencias pr√°cticas: Proxmox vs otros

- **Vs VMware ESXi:** Proxmox es open-source, combina VMs y containers, integra ZFS/Ceph f√°cilmente; menor coste de licencias.
- **Vs soluciones cloud:** Proxmox es para infra on-premise o colocation, ideal para POCs, SMB y datacenters privados.

### 13) Comandos ‚Äúcheat-sheet‚Äù (√∫tiles)

```bash
# VMs / containers
qm list                      # listar VMs (QEMU)
pct list                     # listar containers (LXC)

qm start 100                 # arrancar VM
qm stop 100                  # apagar VM (force)
pct start 101                # iniciar container

qm migrate 100 node2 --online
vzdump 100 --storage backup --compress zstd
qmrestore <backupfile> <newid> --storage local-lvm

pvecm create mi-cluster
pvecm add IP_DEL_PRIMARIO
pvesm status                 # estado storages
pvesh get /nodes             # API: listar nodos
```

### 14) Casos de uso t√≠picos

- **Lab / desarrollo:** r√°pidos despliegues de VMs/CTs.
- **SMB / hosting:** infraestructura econ√≥mica y potente.
- **Producci√≥n a escala:** con Ceph, HA y PBS, entornos tolerantes a fallos.
- **Edge / ROBO:** nodos Proxmox con ZFS para replicaci√≥n eficiente.

---

[üîº](#√≠ndice)

---

## **154. ¬øQu√© es la virtualizaci√≥n Proxmox?**

### ¬øQu√© es la **virtualizaci√≥n Proxmox**?

**Resumen corto:**

La _virtualizaci√≥n Proxmox_ se refiere al uso de **Proxmox VE (Virtual Environment)** como plataforma para ejecutar **m√°quinas virtuales (KVM/QEMU)** y **contenedores (LXC)** sobre servidores f√≠sicos, gestionando almacenamiento, redes, backups, clustering y alta disponibilidad desde una √∫nica interfaz web y API. Es una soluci√≥n open-source pensada para labs, entornos de producci√≥n y nubes privadas.

### 1) ¬øC√≥mo funciona (arquitectura esencial)?

- **Host Proxmox (nodo):** sistema Debian con `pve-manager` que orquesta KVM y LXC.
- **KVM/QEMU:** virtualizaci√≥n completa ‚Äî cada VM tiene su kernel y alto aislamiento. Requiere VT-x/AMD-V.
- **LXC:** contenedores basados en namespaces y cgroups del kernel ‚Äî mucho m√°s ligeros; comparten el kernel del host.
- **Storage backends:** `local`, `local-lvm`, **ZFS**, **Ceph/RBD**, NFS, iSCSI, etc.
- **Networking:** bridges (vmbr0), bonding, VLANs, NAT opcional; las VMs/CTs se conectan a bridges que exponen la red f√≠sica.
- **Cluster:** `pvecm` + `corosync` sincronizan configuraci√≥n y permiten migraciones/HA entre nodos.
- **Backups:** `vzdump` integrado; opcional Proxmox Backup Server (PBS) para backups incrementales/deduplicados.

### 2) Diferencia VM (KVM) vs Contenedor (LXC) ‚Äî ¬øcu√°ndo usar cada uno?

- **VM (KVM)**

  - Aislamiento completo, kernel propio.
  - Ideal para: bases de datos, sistemas con requisitos kernel espec√≠ficos, appliances, VMs Windows.
  - Sobrecarga moderada (CPU/memoria/disk).

- **Contenedor (LXC)**

  - Ligero, arranque r√°pido, menos uso de recursos.
  - Ideal para: microservicios, servicios Linux que no requieren kernel diferente, entornos de CI/CD.
  - Menos aislamiento (mismo kernel del host).

**Ejemplo pr√°ctico:** front-end en LXC (ligero), base de datos en VM (aislada y con tuning de kernel).

### 3) Ejemplos pr√°cticos ‚Äî comandos y flujos comunes

#### Crear una VM (KVM) con `qm`

```bash
# 1) Crear VM (ID 100)
qm create 100 --name web01 --memory 4096 --cores 2 --net0 virtio,bridge=vmbr0

# 2) A√±adir disco (32GB en local-lvm)
qm set 100 --scsihw virtio-scsi-pci --scsi0 local-lvm:32

# 3) Adjuntar ISO (suponiendo que subiste ubuntu-22.04.iso a storage 'local')
qm set 100 --ide2 local:iso/ubuntu-22.04.iso,media=cdrom

# 4) Arrancar
qm start 100
```

#### Crear un contenedor LXC con `pct`

```bash
# Crear CT id 101 desde plantilla subida a local:vztmpl
pct create 101 local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.gz \
  --hostname ct-web --cores 1 --memory 1024 --rootfs local-lvm:8 \
  --net0 name=eth0,bridge=vmbr0,ip=dhcp --start 1
```

#### Snapshot / Backup / Restore

```bash
# Snapshot VM
qm snapshot 100 pre-update --description "Antes de actualizar"

# Backup (vzdump)
vzdump 100 --mode snapshot --compress zstd --storage backup

# Restaurar backup en nuevo ID 200
qmrestore /var/lib/vz/dump/vzdump-qemu-100-2025_09_02.vma.zst 200 --storage local-lvm
```

#### Migraci√≥n en vivo (online migrate)

```bash
# Mover VM 100 al nodo destino (requiere storage compartido o Storage migration)
qm migrate 100 nodo-destino --online
```

#### Crear ZFS pool (ejemplo r√°pido)

```bash
# Crear pool ZFS espejo
zpool create -f rpool mirror /dev/sda /dev/sdb
# Proxmox detectar√° rpool y podr√°s usarlo como storage
```

### 4) Networking: ejemplo de bridge en `/etc/network/interfaces`

```text
auto vmbr0
iface vmbr0 inet static
    address 192.168.10.10/24
    gateway 192.168.10.1
    bridge-ports eno1
    bridge-stp off
    bridge-fd 0
```

- Las VMs con `bridge=vmbr0` reciben IP en la LAN f√≠sica si est√° en modo bridged.

### 5) Cl√∫ster y alta disponibilidad (breve)

- **Crear cl√∫ster (nodo maestro):**

```bash
pvecm create mi-cluster
```

- **Agregar nodo secundario (desde el secundario):**

```bash
pvecm add IP_DEL_PRIMARIO
```

- **Habilitar HA para VM** desde GUI ‚Üí Datacenter ‚Üí HA o con `ha-manager` en CLI. Requiere almacenamiento compartido o Ceph para reiniciar VM en otro nodo.

### 6) Storage: opciones y recomendaciones

- **local-lvm:** pr√°ctico pero no compartido entre nodos.
- **ZFS (local):** snapshots nativos, compresi√≥n, send/receive (buena para backups/replicaci√≥n).
- **Ceph/RBD:** almacenamiento distribuido para cl√∫steres con tolerancia a fallos y alta disponibilidad.
- **NFS / iSCSI:** simples de integrar como storage compartido.

**Buen consejo:** para cl√∫steres de producci√≥n usa Ceph o un NFS/ISCSI compartido para poder migrar/recuperar VMs entre nodos.

### 7) Backups con Proxmox Backup Server (PBS)

- PBS provee backups incrementales, deduplicaci√≥n y restauraciones r√°pidas.
- Flujo t√≠pico: Proxmox VE ‚Üí tarea de backup ‚Üí PBS almacena deduplicado y versionado.
- Recomendado para entornos con muchas VMs/CTs.

### 8) Buenas pr√°cticas / recomendaciones operativas

- **Instalaci√≥n:** usa ISO oficial y, si vas a producci√≥n, considera ZFS para root o LVM + Ceph para datos.
- **Red:** separa management, storage y VM traffic en NICs/VLANs.
- **Drivers:** en VMs Linux, usar `virtio` (paravirtual) para NIC y SCSI.
- **Snapshots:** √∫tiles para checkpoints; no sustituye backups. No los mantengas mucho tiempo.
- **Backups:** automatiza con `vzdump` o PBS; prueba restores.
- **Seguridad:** 2FA en GUI, SSH keys, firewalls por VM, limitar acceso al puerto 8006.
- **Monitorizaci√≥n:** integra Prometheus/Grafana o usa las m√©tricas integradas.

### 9) Casos de uso reales y ejemplos de arquitectura

- **Peque√±a empresa (2 nodos):** ZFS local + NFS para backups ‚Üí correr VMs de servicios internos; replicaci√≥n manual con `zfs send` para DR.
- **Mediana empresa (3+ nodos):** cluster Proxmox + Ceph OSDs en cada nodo ‚Üí VMs con HA, live-migrate, almacenamiento distribuido.
- **Lab / desarrollo:** un solo nodo Proxmox con LXC para microservicios y VMs para bases de datos.

**Ejemplo concreto de despliegue:**

- Nodo1, Nodo2, Nodo3 (cada uno 32GB RAM, SSDs)
- Ceph OSDs en discos de datos ‚Üí pool RBD
- VMs cr√≠ticas (DB) en KVM con discos sobre RBD, HA activado.
- Containers web en LXC sobre local-zfs, backups diarios en PBS.

### 10) Problemas frecuentes y soluciones r√°pidas

- **VM no arranca por falta de espacio:** revisa `pvesm status` y espacio de datastores.
- **Lento I/O:** usar discos NVMe/SSD, comprobar queue depth, usar `virtio` y PVSCSI.
- **Errores de migraci√≥n:** comprobar versiones de Proxmox/CEPH y conectividad entre nodos (ping, ports corosync).
- **Plantillas/ISO no aparecen:** verificar storage y permisos; usar GUI ‚Üí Storage ‚Üí ISO Images ‚Üí Upload.

### Resultado final ‚Äî ¬øqu√© te llevas?

- **Proxmox VE** es una plataforma integral para virtualizaci√≥n (VMs + containers) con GUI, API, clustering y soporte para almacenamiento moderno (ZFS, Ceph).
- Es **flexible** (labs ‚Üí producci√≥n), **econ√≥mico** (open source) y **potente** (migraci√≥n en vivo, HA, backups avanzados con PBS).
- Con los comandos `qm`, `pct`, `pvecm`, `vzdump` puedes automatizar todo desde CLI; la GUI facilita la operativa diaria.

---

[üîº](#√≠ndice)

---

| **Inicio**         | **atr√°s 3**         | **Siguiente 5**           |
| ------------------ | ------------------- | ------------------------- |
| [üè†](../README.md) | [‚è™](./6_3_ESXi.md) | [‚è©](./6_5_Hypervisor.md) |
