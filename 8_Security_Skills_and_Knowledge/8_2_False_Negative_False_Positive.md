| **Inicio**         | **atr√°s 1**                                         | **Siguiente 3**                            |
| ------------------ | --------------------------------------------------- | ------------------------------------------ |
| [üè†](../README.md) | [‚è™](./8_1_Blue_Team_vs_Red_Team_vs_Purple_Team.md) | [‚è©](./8_3_True_Negative_True_Positive.md) |

---

## **√çndice**

| Temario                                                                                                        |
| -------------------------------------------------------------------------------------------------------------- |
| [201. Diferencia entre falso positivo y falso negativo](#201-diferencia-entre-falso-positivo-y-falso-negativo) |
| [202. ¬øQu√© es un virus falso positivo?](#202-qu√©-es-un-virus-falso-positivo)                                   |
| [203. False positives and false negatives](#203-false-positives-and-false-negatives)                           |

# **False Negative / False Positive**

## **201. Diferencia entre falso positivo y falso negativo**

![False Negative / False Positive](/img/8_Security_Skills_and_Knowledge/False_Negative_False_Positive.jpg "False Negative / False Positive")

### ‚úÖ Falso Positivo

Un **falso positivo** ocurre cuando el sistema **detecta una amenaza o condici√≥n que en realidad no existe**.

üìå Ejemplo en ciberseguridad:

- Un antivirus bloquea un archivo leg√≠timo pensando que es malware.
- Un SIEM genera una alerta de ataque, pero solo era tr√°fico normal de red.

üëâ Problema: genera **alertas innecesarias** que pueden causar p√©rdida de tiempo y recursos.

### ‚ùå Falso Negativo

Un **falso negativo** ocurre cuando el sistema **no detecta una amenaza real** (pasa desapercibida).

üìå Ejemplo en ciberseguridad:

- Un malware entra en la red pero el antivirus no lo identifica.
- Un ataque de phishing llega al correo y no es marcado como sospechoso.

üëâ Problema: es **m√°s peligroso** porque permite que el ataque ocurra sin que nadie lo note.

### üîÑ Diferencia entre Falso Positivo y Falso Negativo

| Aspecto          | Falso Positivo                                        | Falso Negativo                          |
| ---------------- | ----------------------------------------------------- | --------------------------------------- |
| Definici√≥n       | Alerta incorrecta: detecta algo que no es una amenaza | No alerta: deja pasar una amenaza real  |
| Efecto           | Sobrecarga de alertas, p√©rdida de tiempo              | Brechas de seguridad, da√±os potenciales |
| Ejemplo          | Antivirus marca un programa seguro como virus         | Antivirus no detecta un troyano real    |
| Riesgo principal | Fatiga de alertas (ignorar amenazas reales)           | Compromiso de sistemas sin detecci√≥n    |

### üìå Resumen

- **Falso positivo**: el sistema ve un ataque que no existe. ‚Üí Molesto, pero no cr√≠tico.
- **Falso negativo**: el sistema no ve un ataque real. ‚Üí Muy peligroso.

üëâ En seguridad, **los falsos negativos son m√°s graves**, pero demasiados falsos positivos tambi√©n afectan porque el equipo puede **ignorar alertas reales** (fatiga de alertas).

---

[üîº](#√≠ndice)

---

## **202. ¬øQu√© es un virus falso positivo?**

Un **falso positivo** en el contexto antivirus ocurre cuando un motor de detecci√≥n marca **un archivo, programa, script o actividad leg√≠tima** como si fuera malware (virus, troyano, ransomware, etc.), pero en realidad **no hay infecci√≥n ni intenci√≥n maliciosa**. Es decir: el sistema ‚Äúalarma‚Äù cuando no deber√≠a.

### ¬øPor qu√© ocurren los falsos positivos? ‚Äî Causas comunes

- **Firmas demasiado gen√©ricas:** una regla de firmas que coincide con c√≥digo com√∫n provoca detecciones en software leg√≠timo.
- **Heur√≠sticas y detecci√≥n basada en comportamiento:** actividades sospechosas (p. ej. modificaci√≥n masiva de archivos, ejecuci√≥n de PowerShell codificado) pueden parecer maliciosas aunque sean administrativas.
- **Ofuscaci√≥n / empaquetado (packers):** binarios comprimidos u ofuscados (para proteger IP) suelen parecer malware porque los atacantes usan t√©cnicas similares.
- **Herramientas leg√≠timas ‚Äúdual-use‚Äù:** utilidades de administraci√≥n (PsExec, herramientas de debugging, Sysinternals) pueden usarse tanto para administrar como para atacar. Algunos AVs las marcan.
- **Actualizaciones defectuosas de la base de firmas:** una actualizaci√≥n de definiciones con un error puede generar detecciones masivas equivocadas.
- **Modelos ML sesgados:** detectores con machine learning pueden aprender patrones que incluyen software leg√≠timo.
- **Scripts/macros corporativos:** macros de Office o scripts PowerShell dise√±ados para automatizar tareas internas pueden disparar alertas.
- **Reglas YARA o reglas personalizadas mal calibradas** en el SOC.

### Ejemplos concretos (reales/hipot√©ticos) ‚Äî para entender mejor

1. **Instalador nuevo de software interno**: un .exe generado por tu equipo de desarrollo (no firmado) es subido al servidor y al instalarlo, el antivirus lo marca como ‚ÄúTrojan.Gen‚Äù por contener cierto patr√≥n de bytes similar a una firma conocida.
2. **Script PowerShell de administraci√≥n**: un script que descarga parches desde un repositorio y ejecuta tareas se detecta como ‚ÄúDownloader‚Äù por usar t√©cnicas de descarga/execuci√≥n din√°mica.
3. **Herramienta de diagn√≥stico**: una utilidad de Sysinternals o una build de PuTTY distribuida internamente es identificada como ‚Äúhacktool‚Äù porque contiene APIs que suelen usar los atacantes.
4. **Macro de Excel leg√≠tima**: plantilla conteniendo macros para generar reportes ser√° marcada como ‚Äúmacro malware‚Äù si la heur√≠stica bloquea macros por defecto.
5. **Actualizaci√≥n de firmas defectuosa**: tras un update del motor AV, cientos de PCs reportan que Chrome o Outlook son maliciosos (caso t√≠pico de update problem√°tico).

> Nota: los ejemplos anteriores son de uso ilustrativo. En incidentes reales conviene confirmar con hashes y an√°lisis adicionales.

### ¬øQu√© riesgos ocasiona un falso positivo?

- **Interrupci√≥n del negocio**: aplicaciones cr√≠ticas bloqueadas o removidas.
- **P√©rdida de productividad**: usuarios y equipos de soporte saturados.
- **Decisiones peligrosas**: eliminaci√≥n masiva de archivos leg√≠timos si se conf√≠a ciegamente en la alerta.
- **Fatiga de alertas**: el SOC puede empezar a ignorar alertas por demasiados falsos positivos.
- **Impacto reputacional**: clientes/usuarios afectados por fallas.

### C√≥mo investigar un posible falso positivo ‚Äî paso a paso (checklist)

1. **Aislar / poner en cuarentena** (si existe riesgo): no ejecutar el archivo en producci√≥n.
2. **Obtener el hash del archivo**

   - Windows PowerShell: `Get-FileHash -Algorithm SHA256 C:\ruta\archivo.exe`
   - Linux/macOS: `sha256sum archivo.bin` o `shasum -a 256 archivo.bin`

3. **Buscar el hash en servicios multi-AV** (VirusTotal u otros) para ver cu√°ntos motores lo detectan y con qu√© nombre.
4. **Comprobar la firma digital** (si aplica)

   - PowerShell: `Get-AuthenticodeSignature C:\ruta\archivo.exe`

5. **Analizar comportamiento en sandbox** (aislado): ejecuci√≥n controlada para ver llamadas a red, creaci√≥n/alteraci√≥n de archivos, procesos hijos.
6. **Revisar contexto y telemetr√≠a EDR**: procesos padres, comandos, IPs de conexi√≥n, persistencia.
7. **Reproducir en entorno de pruebas** si se necesita m√°s evidencia.
8. **Comparar con builds conocidos**: ¬øes una build reciente? ¬øcambi√≥ el empaquetado?
9. **Contactar al vendor del AV**: enviar muestra + hashes + logs + pasos para reproducir (muchos vendors tienen formularios de ‚Äúfalse positive submission‚Äù).
10. **Si urgente y seguro**: crear excepci√≥n temporal (whitelist) en EDR/AV mientras se valida con vendor ‚Äî **documentar y auditar** la excepci√≥n.
11. **Comunicar** a usuarios/claves afectadas y registrar lecciones aprendidas.

### Ejemplo de flujo de resoluci√≥n (mini-caso)

Detecci√≥n: el SOC reporta que el instalador del ERP fue puesto en cuarentena.

Triage: se calcula SHA256 y se consulta VirusTotal ‚Üí 2/70 motores lo marcan (posible FP).

Sandbox: ejecuci√≥n en entorno controlado muestra **ninguna** actividad de exfiltraci√≥n ni persistencia.

Firma: el instalador no est√° firmado (riesgo mayor de FP).

Acci√≥n: se abre ticket con el vendor AV y se agrega una excepci√≥n temporal para los servidores de actualizaci√≥n si el riesgo es alto.

Resoluci√≥n: vendor confirma falso positivo tras an√°lisis y publica actualizaci√≥n; se elimina la excepci√≥n y se documenta el incidente.

### Buenas pr√°cticas para **evitar** falsos positivos (desarrollo y operaciones)

Para desarrolladores y equipos IT:

- **Firmar digitalmente** binarios e instaladores con certificados v√°lidos (code signing).
- **Evitar empaquetadores/obfuscators** innecesarios en builds de producci√≥n; si se usan, documentarlo.
- **Incluir informaci√≥n clara en instaladores** (publisher, checksum, URL oficial).
- **Probar builds contra motores AV** (multi-engine scan) en la pipeline CI antes de release.
- **Distribuir builds desde repositorios confiables** (HTTPS, repos privados).
- **Mantener una pol√≠tica de whitelisting/allowlisting** para herramientas internas; catalogar herramientas admin.
- **Tener procedimientos de rollback y comunicaci√≥n** para cuando un update de firmas cause problemas.
- **Enviar muestras a vendors anticipadamente** si se va a distribuir software inusual.

### Recomendaciones para SOC/administradores

- No borrar archivos masivamente: preferir cuarentena y an√°lisis.
- Mantener un canal/documentaci√≥n para reportar FPs a vendors y seguimiento.
- Automatizar comprobaciones de hashes y b√∫squeda en servicios de reputaci√≥n.
- Configurar exclusiones con control y auditor√≠a (excepciones temporales con ticket).
- Implementar pruebas de actualizaciones de firmas en un entorno piloto antes de desplegar globalmente.

### Resumen (en 3 frases)

Un **virus falso positivo** es cuando una soluci√≥n de seguridad marca como malicioso algo que es leg√≠timo. Sucede por heur√≠sticas, firmas gen√©ricas, ofuscaci√≥n o errores de actualizaci√≥n y puede causar interrupciones importantes si no se gestiona bien. La respuesta correcta combina **triage t√©cnico** (hashes, sandbox, EDR) + **comunicaci√≥n con el vendor** + medidas de mitigaci√≥n controladas (cuarentena o whitelist temporal).

---

[üîº](#√≠ndice)

---

## **203. False positives and false negatives**

### 1) Definiciones sencillas

- **Falso positivo (FP):** el sistema **avisa** que hay un problema (p. ej. malware, spam, enfermedad), pero **en realidad no lo hay**.
- **Falso negativo (FN):** el sistema **no avisa** aunque **s√≠ existe** el problema (p. ej. malware pasa desapercibido, enfermedad no detectada).

En resumen:

- FP = alarma falsa ‚Üí perturbaci√≥n/ruido.
- FN = fallo silencioso ‚Üí riesgo real.

### 2) Confusion matrix (matriz de confusi√≥n)

|                   |   Predicho Positivo |   Predicho Negativo |
| ----------------- | ------------------: | ------------------: |
| **Real Positivo** |  TP (True Positive) | FN (False Negative) |
| **Real Negativo** | FP (False Positive) |  TN (True Negative) |

F√≥rmulas √∫tiles:

- Precision (precisi√≥n) = TP / (TP + FP)
- Recall (sensibilidad) = TP / (TP + FN) (tambi√©n llamado _sensitivity_)
- Specificity (especificidad) = TN / (TN + FP)
- FPR (tasa de falsos positivos) = FP / (FP + TN) = 1 ‚àí specificity
- FNR (tasa de falsos negativos) = FN / (FN + TP) = 1 ‚àí recall
- Accuracy = (TP + TN) / Total

### 3) Ejemplo num√©rico ‚Äî paso a paso (c√°lculos **digit-by-digit**)

Supongamos un detector que clasifica 1000 muestras:

- Total = 1000
- Reales positivos = 150
- Reales negativos = 850
- Predichos positivos = 180
- Verdaderos positivos (TP) = 120

Calculamos FP, FN, TN:

- **FN** = reales positivos ‚àí TP = 150 ‚àí 120 = 30.
- **FP** = predichos positivos ‚àí TP = 180 ‚àí 120 = 60.
- **TN** = reales negativos ‚àí FP = 850 ‚àí 60 = 790.

Comprobaci√≥n totales:

TP + FN = 120 + 30 = 150 (ok).

FP + TN = 60 + 790 = 850 (ok).

TP + TN + FP + FN = 120 + 790 + 60 + 30 = 1000 (ok).

Ahora m√©tricas:

**Accuracy** = (TP + TN) / Total
\= (120 + 790) / 1000
\= 910 / 1000
\= 0.91 ‚Üí **91.00%**

**Precision** = TP / (TP + FP)
\= 120 / (120 + 60)
\= 120 / 180.

Divido 120 entre 180: puedo simplificar dividiendo num. y den. por 60 ‚Üí 120/60 = 2, 180/60 = 3 ‚Üí 2/3.
2/3 = 0.666666... ‚Üí redondeando al 2.¬∫ decimal ‚Üí **66.67%**

**Recall (Sensibilidad)** = TP / (TP + FN)
\= 120 / (120 + 30)
\= 120 / 150.

Divido 120 entre 150: simplifico dividiendo por 30 ‚Üí 120/30 = 4, 150/30 = 5 ‚Üí 4/5 = 0.8 ‚Üí **80.00%**

**Specificity (Especificidad)** = TN / (TN + FP)
\= 790 / (790 + 60)
\= 790 / 850.

Simplifico dividiendo por 10 ‚Üí 79 / 85.

Ahora hago la divisi√≥n larga (79 √∑ 85):

- 85 √ó 0 = 0 ‚Üí resto 79 ‚Üí decimales.
- 790 √∑ 85 = 9 (porque 85√ó9=765); resto 790‚àí765 = 25 ‚Üí 0.9...
- 250 √∑ 85 = 2 (2√ó85=170); resto 250‚àí170 = 80 ‚Üí 0.92...
- 800 √∑ 85 = 9 (9√ó85=765); resto 800‚àí765 = 35 ‚Üí 0.929...
- 350 √∑ 85 = 4 (4√ó85=340); resto 10 ‚Üí 0.9294...
  Continuando ‚Üí 0.9294117... ‚Üí redondeando al 2.¬∫ decimal ‚Üí **92.94%**

**FPR (tasa de falsos positivos)** = FP / (FP + TN)
\= 60 / (60 + 790) = 60 / 850.
Divido por 10 ‚Üí 6/85. Divisi√≥n: 6 √∑ 85 = 0.0705882... ‚Üí **7.06%** (aprox.)

**FNR (tasa de falsos negativos)** = FN / (FN + TP)
\= 30 / (30 + 120) = 30 / 150 = 1/5 = **20.00%**

Relaciones f√°ciles:

- FPR ‚âà **7.06%**, Specificity = **92.94%** (porque Specificity = 1 ‚àí FPR).
- FNR = **20%**, Recall = **80%** (porque Recall = 1 ‚àí FNR).

### 4) Ejemplos reales por dominio

**Ciberseguridad (IDS / AV)**

- FP: el IDS marca tr√°fico de backup leg√≠timo como ‚Äúcomando remoto malicioso‚Äù ‚Üí muchas alertas innecesarias ‚Üí _alert fatigue_.
- FN: un exploit real pasa sin ser detectado ‚Üí atacante logra persistencia ‚Üí **alto riesgo**.

**Machine Learning (clasificador de spam)**

- FP: correo leg√≠timo llega a la carpeta spam ‚Üí p√©rdida de comunicaci√≥n importante.
- FN: un correo phishing llega a la bandeja principal ‚Üí riesgo de compromiso.

**Medicina (test diagn√≥stico)**

- FP: test detecta enfermedad en sano ‚Üí ansiedad, pruebas innecesarias.
- FN: test negativo en persona enferma ‚Üí falta de tratamiento ‚Üí **consecuencia grave**.

  En medicina se suele priorizar baja FNR (alta sensibilidad) cuando perder un caso es cr√≠tico.

**QA / pruebas de software**

- FP: test autom√°tico falla por un flaky test ‚Üí gasto de tiempo investigando bug que no existe.
- FN: un bug real no es detectado ‚Üí falla en producci√≥n.

### 5) Trade-offs y decisiones (¬øqu√© priorizar?)

- **Bajar umbral de detecci√≥n** ‚Üí disminuye FNs (mejora recall) pero aumenta FPs (peor precision).
- **Subir umbral** ‚Üí menos FPs pero m√°s FNs.
  La elecci√≥n depende del **coste relativo**: perder un caso (FN) puede ser m√°s caro que tratar una falsa alarma (FP), o viceversa. Ejemplos:
- Medicina grave ‚Üí priorizar **baja FNR** (sensitivity alta).
- Sistemas con capacidad humana limitada (SOC peque√±o) ‚Üí intentar **reducir FPs** para evitar fatiga.

Herramientas de an√°lisis: **ROC curve** (trade-off TPR vs FPR), **Precision-Recall curve** (mejor cuando los positivos son raros).

### 6) C√≥mo reducir falsos positivos y falsos negativos (pr√°ctico)

**Reducir falsos positivos (menos ruido):**

- Afinar reglas / firmas (menos gen√©ricas).
- Enriquecer alertas con contexto (telemetr√≠a, reputaci√≥n IP, proceso padre).
- Whitelist / allowlist para herramientas leg√≠timas (con control y auditor√≠a).
- Calibrar modelos ML, usar validaci√≥n cruzada y m√°s datos representativos.
- Implementar verificaci√≥n humana para alertas dudosas (human-in-loop).

**Reducir falsos negativos (no dejar pasar amenazas):**

- A√±adir detecci√≥n basada en comportamiento (EDR/XDR) adem√°s de firmas.
- Bajar umbral en escenarios cr√≠ticos o usar m√∫ltiples detectores en paralelo.
- Aumentar diversidad y calidad del set de entrenamiento (en ML).
- Implementar detecci√≥n en capas (network + host + application).
- Usar threat intelligence y se√±ales externas para enriquecer detecci√≥n.

**En ML:**

- Rebalancear clases (oversampling/undersampling), penalizar errores en la funci√≥n de p√©rdida (cost-sensitive learning).
- Ensembles y stacking suelen mejorar robustez frente a FNs/FPs.

### 7) Recomendaciones pr√°cticas (resumen accionable)

- Define **qu√© coste tiene un FP vs un FN** en tu contexto.
- Mide siempre **precision & recall** y usa PR-curve para datasets desbalanceados.
- No ignores las alertas por muchos FPs: instrumenta triage autom√°tico y escalado.
- Ten procesos para **reportar y corregir falsos positivos** (p. ej. enviar muestras a vendor AV).
- Usa _purple teaming_ (Rojo + Azul) para probar detecci√≥n realista y reducir FNs.

### 8) Resumen r√°pido

- **Falso positivo = alarma donde no hay problema** (ruido).
- **Falso negativo = problema que no se detect√≥** (peligro).
- Prioriza reducir FNs cuando las consecuencias de ‚Äúno detectar‚Äù son graves; prioriza reducir FPs cuando el volumen de alertas impide la operaci√≥n.
- M√©tricas clave: **precision, recall, specificity, FPR, FNR** ‚Äî mide y elige umbrales seg√∫n coste.

---

[üîº](#√≠ndice)

---

| **Inicio**         | **atr√°s 1**                                         | **Siguiente 3**                            |
| ------------------ | --------------------------------------------------- | ------------------------------------------ |
| [üè†](../README.md) | [‚è™](./8_1_Blue_Team_vs_Red_Team_vs_Purple_Team.md) | [‚è©](./8_3_True_Negative_True_Positive.md) |
