| **Inicio**         | **atr√°s 2**                                  | **Siguiente 4**                             |
| ------------------ | -------------------------------------------- | ------------------------------------------- |
| [üè†](../README.md) | [‚è™](./8_2_False_Negative_False_Positive.md) | [‚è©](./8_4_Basics_of_Threat_Intel_OSINT.md) |

---

## **√çndice**

| Temario                                                                                                                                                  |
| -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [204. Falsos positivos y falsos negativos en la seguridad de la informaci√≥n](#204-falsos-positivos-y-falsos-negativos-en-la-seguridad-de-la-informaci√≥n) |
| [205. Falsos positivos y falsos negativos](#205-falsos-positivos-y-falsos-negativos)                                                                     |

# **Verdadero negativo / Verdadero positivo**

## **204. Falsos positivos y falsos negativos en la seguridad de la informaci√≥n**

![Verdadero negativo / Verdadero positivo](/img/8_Security_Skills_and_Knowledge/Verdadero_negativo_Verdadero_positivo.png "Verdadero negativo / Verdadero positivo")

### üìå Descripci√≥n general

En ciberseguridad, los sistemas de detecci√≥n como **antivirus, EDR, IDS/IPS, SIEM o firewalls** deben diferenciar entre **actividades leg√≠timas** y **amenazas reales**.

- Cuando el sistema se equivoca, genera **falsos positivos** o **falsos negativos**.
- Encontrar un equilibrio entre ambos es clave para la **eficacia y eficiencia** de un SOC (Security Operations Center).

### ‚úÖ Falsos positivos: ¬øqu√© son?

Un **falso positivo** ocurre cuando una herramienta de seguridad **marca una actividad leg√≠tima como maliciosa**.

üìå Ejemplos:

- Un antivirus bloquea un instalador oficial porque lo confunde con un troyano.
- Un IDS detecta tr√°fico normal de backup como un ataque de exfiltraci√≥n.
- Un correo corporativo leg√≠timo llega a la carpeta de spam.

üëâ Aunque no representan una amenaza real, consumen tiempo y recursos del equipo de seguridad.

### üîç C√≥mo identificar mejor los falsos positivos

- **Correlaci√≥n de alertas:** comparar con otras fuentes de telemetr√≠a (logs de endpoints, firewalls, SIEM).
- **Contexto del sistema:** verificar qu√© usuario/proceso origin√≥ la actividad.
- **An√°lisis de threat intelligence:** comprobar reputaci√≥n de IPs, dominios o hashes.
- **Whitelisting / Allowlisting:** marcar aplicaciones leg√≠timas para que no se repitan falsas alarmas.

### ‚ùå ¬øQu√© son los falsos negativos?

Un **falso negativo** ocurre cuando una amenaza real **no es detectada** por el sistema.

üìå Ejemplos:

- Un malware polim√≥rfico evade las firmas del antivirus.
- Un atacante realiza movimiento lateral sin generar alertas.
- Un correo phishing pasa desapercibido por el filtro de spam.

üëâ Son **m√°s peligrosos** que los falsos positivos, porque el ataque ocurre sin ser detectado.

### ‚ö†Ô∏è Las consecuencias de los falsos negativos

- **Intrusiones exitosas:** robo de datos, ransomware, persistencia del atacante.
- **Da√±os financieros:** multas, p√©rdida de clientes y reputaci√≥n.
- **Detecci√≥n tard√≠a:** cuanto m√°s tarde se descubre, m√°s costosa es la remediaci√≥n.

### üîß C√≥mo reducir los falsos negativos

- **Defensa en capas (defense in depth):** combinar detecci√≥n basada en firmas, comportamiento y anomal√≠as.
- **Uso de inteligencia de amenazas:** actualizar constantemente indicadores de compromiso (IOCs).
- **Simulaciones de ataque (Red Teaming / Purple Teaming):** probar la eficacia real de las defensas.
- **Machine learning adaptativo:** modelos que aprenden de patrones nuevos para no depender solo de firmas.

### üòµ‚Äçüí´ El coste de la fatiga por alerta

La **fatiga por alerta** ocurre cuando los analistas reciben demasiados falsos positivos.

- Se pierde tiempo revisando incidentes que no son amenazas.
- El equipo puede **empezar a ignorar alertas reales**.
- Aumenta el **burnout del personal de SOC**, lo que impacta en la rotaci√≥n de empleados.

### üíº Impacto empresarial de la fatiga por alertas

- **Productividad reducida:** los analistas dedican horas a alarmas irrelevantes.
- **Costos de operaci√≥n elevados:** m√°s personal necesario para filtrar ruido.
- **Mayor riesgo de brechas:** amenazas cr√≠ticas pueden pasar desapercibidas.
- **Decisiones err√≥neas:** se pierde confianza en las herramientas de seguridad.

### üõ†Ô∏è C√≥mo reducir las falsas alarmas

- **Optimizar reglas de detecci√≥n:** ajustar firmas y pol√≠ticas de seguridad.
- **Integraci√≥n de fuentes de datos:** enriquecer alertas con contexto (usuario, host, historial).
- **Automatizaci√≥n (SOAR):** playbooks que clasifican y descartan falsos positivos autom√°ticamente.
- **Entrenamiento del personal:** mejorar la capacidad de an√°lisis de los equipos SOC.

### ‚öôÔ∏è Optimizar la pila de tecnolog√≠a de ciberseguridad

- Consolidar herramientas para evitar **alertas duplicadas**.
- Escoger soluciones con **detecci√≥n basada en comportamiento** adem√°s de firmas.
- Priorizar plataformas con **an√°lisis autom√°tico y machine learning**.
- Evaluar la integraci√≥n con SIEM y SOAR para centralizar correlaci√≥n.

### üéöÔ∏è Optimizar los umbrales de alerta

El umbral de detecci√≥n determina el balance entre **sensibilidad** y **precisi√≥n**.

- **Umbral bajo:** detecta m√°s amenazas (menos falsos negativos) pero genera m√°s falsos positivos.
- **Umbral alto:** reduce ruido (menos falsos positivos) pero aumenta el riesgo de falsos negativos.

üìå Estrategia:

- Ajustar umbrales seg√∫n el **riesgo empresarial** (ej. sistemas cr√≠ticos ‚Üí tolerancia cero a FNs).
- Revisar continuamente los par√°metros a medida que cambian los patrones de ataque.

### üìå Conclusi√≥n

- **Falsos positivos = ruido** ‚Üí consumen tiempo y generan fatiga.
- **Falsos negativos = ceguera** ‚Üí permiten ataques reales.
- La clave est√° en **optimizar tecnolog√≠a + procesos + personal** para reducir ambos.
- Empresas exitosas logran un equilibrio: **m√≠nimos falsos negativos sin inundar al equipo con falsos positivos**.

---

[üîº](#√≠ndice)

---

## **205. Falsos positivos y falsos negativos**

### 1) Concepto simple

- **Falso positivo (FP):** el sistema **avisa que hay una amenaza** cuando en realidad **no la hay**.
  ‚Üí ‚Äú¬°Alarma falsa!‚Äù
- **Falso negativo (FN):** el sistema **no avisa** aunque **s√≠ existe** una amenaza.
  ‚Üí ‚ÄúLa amenaza pasa desapercibida.‚Äù

### 2) Matriz de confusi√≥n (visual)

|                   |   Predicho Positivo |   Predicho Negativo |
| ----------------- | ------------------: | ------------------: |
| **Real Positivo** |  TP (True Positive) | FN (False Negative) |
| **Real Negativo** | FP (False Positive) |  TN (True Negative) |

Donde:

- TP = detect√≥ y era real.
- TN = no detect√≥ y no era amenaza.
- FP = detect√≥ pero no era amenaza.
- FN = no detect√≥ pero s√≠ era amenaza.

### 3) M√©tricas clave (f√≥rmulas)

- **Accuracy** = (TP + TN) / Total
- **Precision** = TP / (TP + FP)
- **Recall (Sensitivity)** = TP / (TP + FN)
- **Specificity** = TN / (TN + FP)
- **FPR (False Positive Rate)** = FP / (FP + TN) = 1 ‚àí Specificity
- **FNR (False Negative Rate)** = FN / (FN + TP) = 1 ‚àí Recall

### 4) Ejemplo num√©rico ‚Äî paso a paso (long division / digit-by-digit)

Supongamos un detector con estos resultados en 1000 eventos:

- TP = 160
- FP = 60
- FN = 40
- TN = 740
- Total = 160 + 60 + 40 + 740 = 1000 (comprobado)

Calculamos m√©tricas:

**Accuracy**

- Suma TP + TN = 160 + 740 = 900.
- Accuracy = 900 / 1000 = 0.9 ‚Üí **90.00%**

**Precision**

- Denominador TP + FP = 160 + 60 = 220.
- Precision = 160 / 220.
  Simplifico: dividir numerador y denominador entre 20 ‚Üí 160/20 = 8 ; 220/20 = 11 ‚Üí 8/11.
  Divisi√≥n larga 8 √∑ 11:

  - 11 √ó 0 = 0 ‚Üí resto 8 ‚Üí punto decimal.
  - 80 √∑ 11 = 7 (7√ó11=77) ‚Üí resto 80‚àí77 = 3.
  - 30 √∑ 11 = 2 (2√ó11=22) ‚Üí resto 30‚àí22 = 8.
  - Vuelve a repetirse: 80 ‚Üí 7 ... es peri√≥dico: 0.727272...
    Redondeando a 2 decimales ‚Üí **72.73%**

**Recall (Sensibilidad)**

- Denominador TP + FN = 160 + 40 = 200.
- Recall = 160 / 200 = 16/20 = 4/5 = 0.8 ‚Üí **80.00%**

**Specificity**

- Denominador TN + FP = 740 + 60 = 800.
- Specificity = 740 / 800.
  Simplifico dividiendo por 20 ‚Üí 740/20 = 37 ; 800/20 = 40 ‚Üí 37/40.
  37 √∑ 40 = 0.925 ‚Üí **92.50%**

**FPR (tasa de falsos positivos)**

- FPR = FP / (FP + TN) = 60 / 800 = 6 / 80 = 3 / 40 = 0.075 ‚Üí **7.50%**

**FNR (tasa de falsos negativos)**

- FNR = FN / (FN + TP) = 40 / 200 = 1 / 5 = 0.2 ‚Üí **20.00%**

‚Äî Comprobaciones r√°pidas:

- Precision ‚âà 72.73% (buen equilibrio pero hay ruido).
- Recall = 80% ‚Üí el sistema captura 4 de cada 5 amenazas reales.
- FNR = 20% ‚Üí 1 de cada 5 amenazas pasa desapercibida (riesgo).

### 5) Ejemplos concretos por dominio

**a) Seguridad de la informaci√≥n (IDS, EDR, SIEM)**

- FP: Un proceso leg√≠timo de backup abre muchas conexiones remotas (puede parecer exfiltraci√≥n) ‚Üí IDS genera alerta.
- FN: Un malware polim√≥rfico ejecuta cargas sin firmas conocidas ‚Üí EDR no lo detecta ‚Üí atacante obtiene persistencia.

**b) Machine Learning (clasificadores)**

- FP: Correo leg√≠timo marcado como spam ‚Üí p√©rdida de comunicaci√≥n.
- FN: Phishing que pasa al inbox ‚Üí un empleado puede caer.

**c) Medicina**

- FP: Test da positivo en una persona sana ‚Üí pruebas y ansiedad innecesarias.
- FN: Test no detecta una enfermedad real ‚Üí tratamiento retrasado, consecuencias graves.

**d) QA / testing**

- FP: Test autom√°tico ‚Äúfalla‚Äù por flaky test ‚Üí ingeniero investiga bug inexistente.
- FN: Bug real no cubierto por tests ‚Üí falla en producci√≥n.

### 6) Causas comunes de falsos positivos y falsos negativos (seguridad)

**Falsos positivos**

- Reglas/siganturas gen√©ricas o mal calibradas.
- Heur√≠sticas que confunden admin tools con malware (‚Äúdual-use tools‚Äù).
- Cambios leg√≠timos en software no reconocidos por el motor.
- Actualizaciones defectuosas de firmas.

**Falsos negativos**

- Malware nuevo o ofuscado que evade firmas.
- T√©cnicas de living-off-the-land (usar herramientas leg√≠timas).
- Telas de bajo ruido: actividad que no rompe umbrales.
- Falta de telemetr√≠a (pocos logs, ciegos en hosts).

### 7) Consecuencias operativas y de negocio

- **Coste de los falsos positivos:** tiempo perdido, sobrecarga del SOC, tickets, reducci√≥n de confianza en alertas.
- **Coste de los falsos negativos:** brechas no detectadas, robo/exfiltraci√≥n, ransomware, multas, da√±o reputacional.
- **Fatiga por alertas (alert fatigue):** analistas ignoran alertas tras muchos FPs ‚Üí aumenta probabilidad de FNs efectivos.

### 8) C√≥mo identificar mejor los falsos positivos (buen workflow de triage)

1. **Correlaci√≥n**: Ver si hay alertas relacionadas (firewall, proxy, endpoint).
2. **Contexto**: revisar proceso padre, usuario, horario y servidor origen.
3. **Reputaci√≥n**: comprobar IPs, dominios y hashes en threat intel / VirusTotal.
4. **Sandboxing**: ejecutar muestra en entorno aislado y observar comportamiento.
5. **EDR telemetry**: revisar registros de procesos, conexiones, tasa de creaci√≥n de archivos.
6. **Reproducibilidad**: intentar reproducir la actividad en test controlado.
7. **Contactar vendor**: reportar la muestra al proveedor si hay sospecha de FP.
8. **Documentar y whitelist**: si es leg√≠timo, crear excepci√≥n controlada con ticket y expiraci√≥n.

### 9) C√≥mo reducir falsos negativos (estrategias defensivas)

- **Defense-in-depth**: combinar red + host + aplicaci√≥n + detecci√≥n en la nube.
- **Detecci√≥n basada en comportamiento** (anomal√≠as) adem√°s de firmas.
- **Threat intelligence**: ingesti√≥n continua de IOCs y TTPs.
- **Simulaciones (Red Teams / Purple Teams)**: validar detecciones contra t√©cnicas reales.
- **Mejorar telemetr√≠a**: m√°s logs, mejor retention, instrumentation.
- **Modelos ML mejor entrenados**: datos representativos, reentrenamiento regular.

### 10) C√≥mo reducir falsos positivos (pr√°ctico)

- Afinar reglas (menos gen√©ricas).
- Enriquecer alertas con contexto (usuario, proceso, asset criticality).
- Implementar SOAR para playbooks que filtren/triage autom√°ticamente.
- Implementar whitelisting/allowlisting para herramientas admin conocidas (con control).
- Piloto de actualizaciones de firmas antes de desplegar globalmente.
- Retroalimentaci√≥n continua del SOC al equipo que mantiene las reglas.

### 11) Trade-offs y ajuste de umbrales

- Bajar el umbral ‚Üí menos FNs (mejor recall) pero m√°s FPs (peor precision).
- Subir el umbral ‚Üí menos FPs pero m√°s FNs.
  Herramientas: **ROC curve** (TPR vs FPR) y **Precision-Recall curve** (mejor cuando hay desbalance de clases).
  Decisi√≥n: fijar umbrales seg√∫n **coste relativo** de FN vs FP en tu organizaci√≥n (ej.: para sistemas cr√≠ticos priorizas minimizar FNs).

### 12) Checklist operativo para SOC (acci√≥n inmediata)

- Medir **precision y recall** de reglas cr√≠ticas peri√≥dicamente.
- Mantener m√©tricas de **volumen de alertas** y % de falsos positivos detectados.
- Implementar **playbooks SOAR** para triage autom√°tico.
- Mantener un proceso claro para **reportar FPs** a vendors y documentar excepciones.
- Realizar **purple teaming** trimestralmente en assets cr√≠ticos.

### 13) Resumen ejecutivo (3 frases)

- **Falso positivo** = alarma donde no hay amenaza ‚Üí ruido y coste operativo.
- **Falso negativo** = amenaza no detectada ‚Üí riesgo directo y potencialmente grave.
- La clave es medir (precision/recall), entender el _coste_ de cada error y ajustar tecnolog√≠a/procesos para minimizar el impacto empresarial.

---

[üîº](#√≠ndice)

---

| **Inicio**         | **atr√°s 2**                                  | **Siguiente 4**                             |
| ------------------ | -------------------------------------------- | ------------------------------------------- |
| [üè†](../README.md) | [‚è™](./8_2_False_Negative_False_Positive.md) | [‚è©](./8_4_Basics_of_Threat_Intel_OSINT.md) |
